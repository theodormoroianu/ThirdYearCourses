{"cells":[{"cell_type":"markdown","metadata":{"id":"YNzBsYHukEf-"},"source":["# CNNs pentru clasificarea textelor\n","\n","Folosirea CNN-urilor pentru a clasifica textul a fost prima dată prezentată în: [Convolutional Neural Networks for Sentence Classification](https://aclanthology.org/D14-1181.pdf).\n","\n","### Arhitectura CNN\n","\n","<img src=\"https://richliao.github.io/images/YoonKim_ConvtextClassifier.png\">\n","\n","Dându-se ca input un text de $n$ cuvinte $w_{1}$, $w_{2}$, ..., $w_{n}$, transformăm fiecare cuvânt într-un vector de dimensiune $d$, rezultând vectorii $w_{1}$, $w_{2}$, ..., $w_{n}$ aparținând $R^d$. Matricea rezultată de dimensiune $d$×$n$ este apoi folosită ca input pentru un layer convoluțional care trece un *sliding window* peste text.\n","\n","Pentru fiecare window de lungime $l$:\n","\n","$u_{i}$ = [$w_{i}$, ..., $w_{i+l-1}$] $∈ R^{d×l}$, 0≤$i$≤$n-l$ \n","\n","Pentru fiecare filtru $f_{j} ∈ R^{d×l}$ calculăm <$u_{i}$, $f_{j}$> și obținem matricea $ F ∈ R^{m×n}$ (dacă am făcut padding înainte de aplicarea filtrului, astfel încât să păstrăm dimensiunea $n$ a cuvintelor), unde $m$ este numărul de filtre. Aplicăm max-pooling pe matricea $F$ rezultată, apoi aplicăm funcția de activare. În final, avem un layer *fully connected* care produce distribuția pe clase, din care rezultă clasa cu cea mai mare probabilitate."]},{"cell_type":"markdown","metadata":{"id":"Rn2QNjUCg6z0"},"source":["### Convoluții și filtre\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.001.png\" width=\"300\">\n","\n","**Inputul** este format dintr-o matrice de dimensiune $n$×$d$, unde $n$ este numărul de cuvinte sau caractere, iar $d$ este lungimea reprezentării vectoriale sau lungimea vocabularului.  \n","\n","De exemplu, pentru reprezentarea vectorială a unui text la nivel de caracter, pentru $d$ = 70, numărul de caractere unice în vocabular, pe fiecare linie a matricei avem reprezentarea one-hot a unui caracter.\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.002.png\" width=\"500\">\n","\n","**Filtrele** (kernels) pot avea orice lungime. Lungimea este dată de numărul de linii din filtru. Lățimea filtrului trebuie să fie aceeași cu numărul de coloane din reprezentarea vectorială ($d$).\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.003.png\" width=\"300\">\n","\n","Operația de convoluție presupune multiplicarea elementelor din input și filtru, rezultând o valoare care reprezintă suma rezultatelor multiplicării. În consecință, operația de convoluție multiplică *weight*-urile din filtru cu reprezentarea vectorială a cuvintelor.\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.004.png\" width=\"300\">\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.005.png\" width=\"300\">\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.006.png\" width=\"300\">\n","\n","Filtrul este aplicat secvențial peste input, dar, la fel ca în cazul imaginilor, putem folosi diferite valori pentru *stride* pentru a controla cât de mult de mișcă filtrul vertical. Utilizând *stride* cu o valoare $k$ putem aplica un filtrul din $k$ în $k$ linii. De exemplu, pentru un filtru cu stride = 2, filtrul va fi aplicat pe secvența de text din 2 în 2 linii și vom avea un output de dimensiune mai mică.\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv2.006.png\" width=\"450\">\n","\n","Filtre multiple vor produce output-uri multiple. \n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv2.007.png\" width=\"450\">\n","\n","La următorul pas se realizează *max pooling* peste fiecare feature map rezultat din aplicarea filtrelor, iar apoi rezultatele sunt concatenate. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"T3_PYPN8fajE"},"source":["### Imagini vs Text\n","\n","Pentru a înțelege de ce o abordare folosind CNN-uri este potrivită pentru text, trebuie să ne gândim la textele noastre ca fiind niște imagini.\n","\n","Pentru exemplul următor vom cosidera că reprezentarea unei propoziții a fost făcută la nivel de cuvânt.\n","\n","De exemplu, pentru o propoziție cu lungimea maximă de 70 de cuvinte și lungimea embeddingului egală cu 300, putem crea o matrice cu valori numerice de forma 70x300 pentru a reprezenta această propoziție. Spre deosebire de imagini, în care elementele matricei sunt reprezentate de valori ale pixelilor, fiecare linie din reprezentarea vectorială a propoziției este, de fapt, reprezentarea unui cuvânt.\n","\n","În cazul imaginilor, filtrul de convoluție se deplasează și vertical și orizontal, dar în cazul textului, filtrul se deplasează doar vertical, convoluțiile sunt doar 1D. Un kernel de dimensiune (2, 300), care are dimensiunea filtrului egală cu 2 se uită doar la 2 cuvinte în același timp. Ne putem gândi, deci, la dimensionea filtrelor ca la o dimensiune a n-gramelor (bigrame, trigrame, etc.)."]},{"cell_type":"markdown","metadata":{"id":"4Z5b8qtPXLdB"},"source":["În acest laborator vom folosi datasetul IMDb movie reviews: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7357,"status":"ok","timestamp":1647392850585,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"aSKhMG6_U9Ch","outputId":"f107aac1-8bd5-4c2d-bd77-7a46b2c04fc1"},"outputs":[],"source":["! pip install unidecode"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1647392850586,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"S3RBIBZPU2Qp","outputId":"e116e69b-7df3-4d09-814f-3fc6f1807c8b"},"outputs":[],"source":["import torch\n","import pandas as pd\n","from pprint import pprint\n","from sklearn.model_selection import train_test_split\n","from unidecode import unidecode\n","from collections import Counter\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1647392851637,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"rDEgcB_E8uWL","outputId":"85a8b8ad-a943-4cdd-c823-2a159107558d"},"outputs":[],"source":["from urllib.request import urlretrieve\n","urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":802,"status":"ok","timestamp":1647392852435,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"Q1cush6tEdAk","outputId":"3c6cdf01-a5fe-4c7d-9afb-fab40006d5f5"},"outputs":[],"source":["data = pd.read_csv('IMDB_Dataset.csv')\n","data = data[:10000]\n","data"]},{"cell_type":"markdown","metadata":{"id":"qaZmejBvY2jK"},"source":["Impartim datasetul in train si test."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1647392852435,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"hVFj4JHVY19U","outputId":"a7d98a81-593e-415a-c363-3a9851e55ce8"},"outputs":[],"source":["train_df, test_df = train_test_split(data, test_size=0.20, random_state = 42)\n","\n","print('Dimensiunea datelor de train', len(train_df))\n","print('Dimensiunea datelor de test', len(test_df))"]},{"cell_type":"markdown","metadata":{"id":"4i3KQ4qbkcR3"},"source":["Așa cum am văzut în laboratoarele trecute, nu putem antrena un model direct pe datele sub formă de text, trebuie să transformam datele în reprezentări numerice vectoriale. \n","\n","Pentru asta, trebuie să parcurgem 2 pași:\n","\n","- **Tokenizare**: împărțirea textelor în subtexte mai mici. Astfel vom determina vocabularul setului nostru de date (setul de tokeni unici)\n","\n","- **Vectorizare**: reprezentarea în format numeric vectorial"]},{"cell_type":"markdown","metadata":{"id":"UfqLJKKSlGZ9"},"source":["Textul poate fi reprezentat fie ca o secvență de caractere, fie ca o secvență de cuvinte. Utilizarea reprezentării la nivel de cuvânt are o performanță mai bună și este mai folosită, pe când reprezentarea la nivel de caracter este utilă dacă textele au multe greșeli de scriere. "]},{"cell_type":"markdown","metadata":{"id":"5VNrOeWin9YU"},"source":["### Reprezentarea vectorială la nivel de caracter"]},{"cell_type":"markdown","metadata":{"id":"fmzl7MpVECgF"},"source":["\n","```\n","Texts: 'the mouse ran up the clock' and 'the mouse ran down'\n","```\n","\n","Pe langa caracterele prezente in textele noastre, adaugam si 2 tokeni speciali: UNK (unknown) si PAD.\n","\n","\n","```\n","Index assigned for every token: {0: 'UNK', 1: 'PAD', 2: 't', 3: 'm', 4: 'c', 5: 'h', 6: 'l', 7: 'w', 8: ' ', 9: 'a', 10: 'k', 11: 'e', 12: 'r', 13: 'u', 14: 'n', 15: 's', 16: 'd', 17: 'p', 18: 'o'}\n","```\n","\n","Reprezentarea vectoriala a celor doua texte folosind indexul corespunzator pentru fiecare caracter:\n","\n","```\n","'the mouse ran up the clock' = [2, 5, 11, 8, 3, 18, 13, 15, 11, 8, 12, 9, 14, 8, 13, 17, 8, 2, 5, 11, 8, 4, 6, 18, 4, 10]\n","'the mouse ran down' = [2, 5, 11, 8, 3, 18, 13, 15, 11, 8, 12, 9, 14, 8, 16, 18, 7, 14]\n","```\n","\n","Adaugam valori de padding la cel de-al doilea vector pentru a avea o lungime egala cu primul vector si obtinem:\n","\n","```\n","[2, 5, 11, 8, 3, 18, 13, 15, 11, 8, 12, 9, 14, 8, 16, 18, 7, 14, 1, 1, 1, 1, 1, 1, 1, 1]\n","```"]},{"cell_type":"markdown","metadata":{"id":"EUVIwpq-q1b-"},"source":["Transformăm lista de review-uri într-o listă de caractere pentru fiecare review."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1740,"status":"ok","timestamp":1647392854167,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"S3FfaGAeoKBW","outputId":"886715e1-108d-46f8-de6b-f26ead83e328"},"outputs":[],"source":["def transform_to_char(data):\n","\n","    reviews = []\n","    \n","    for review in data:\n","        review_cleaned = [char.lower() for char in review]\n","        reviews.append(review_cleaned)\n","\n","    return reviews\n","\n","train_reviews = transform_to_char(train_df.review)\n","train_labels = train_df.sentiment.tolist()\n","\n","print('Reviews', len(train_reviews))\n","print('Labels', len(train_labels))"]},{"cell_type":"markdown","metadata":{"id":"z9FozeZ1uyeL"},"source":["Calculam marimea vocabularului de caractere."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2073,"status":"ok","timestamp":1647392856234,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"rjMZZNaEojSq","outputId":"aa944ba3-6cdc-4893-868b-d1bbdee036f1"},"outputs":[],"source":["def get_vocab(data):\n","\n","    units = set([unit for review in data for unit in review])\n","    \n","    return units\n","\n","vocab = get_vocab(train_reviews)\n","\n","print('total chars:', len(vocab))\n","print(vocab)"]},{"cell_type":"markdown","metadata":{"id":"yFpdaZWW3JSj"},"source":["Putem vedea ca avem foarte multe caractere cu accente, diferite de caracterele limbii engleze. Pentru a micsora lungimea vocabularului, putem transforma caracterele utf8 in cea mai apropiata forma ASCII a lor."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4649,"status":"ok","timestamp":1647392860877,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"5Mz0gE3uWNV0","outputId":"3c0dac72-5fa8-4b2a-8fae-caeaf0fb0ab0"},"outputs":[],"source":["reviews_to_ascii = [unidecode(review) for review in train_df.review]\n","train_reviews = transform_to_char(reviews_to_ascii)\n","vocab = get_vocab(train_reviews)\n","\n","print('total chars:', len(vocab))\n","print(vocab)"]},{"cell_type":"markdown","metadata":{"id":"745uYasFu4Eq"},"source":["Atribuim fiecarui caracter din vocabularul nostru un index. Vom atribui 0 caracterelor necunoscute, iar 1 va fi valoarea atribuita paddingului."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1647392860880,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"ntu0UPDNrtHh","outputId":"73113fca-b854-4e67-8a8a-2fdd879cce9e"},"outputs":[],"source":["char_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n","indices_char = dict((i + 2, c) for i, c in enumerate(vocab))\n","\n","indices_char[0] = 'UNK'\n","char_indices['UNK'] = 0\n","\n","indices_char[1] = 'PAD'\n","char_indices['PAD'] = 1\n","\n","print('Dimensiunea vocabularului', len(indices_char))\n","print(indices_char)"]},{"cell_type":"markdown","metadata":{"id":"pXmYEKsUYHNf"},"source":["Acum putem transforma propozitiile din datasetul nostru intr-o reprezentare vectoriala, in care vom avea pentru fiecare caracter indicele corespunzator din vocabularul nostru."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5397,"status":"ok","timestamp":1647392866264,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"D7Xjjjgfsw8S"},"outputs":[],"source":["import numpy as np\n","\n","def vectorize_sentences(data, char_indices, one_hot = False):\n","    vectorized = []\n","    for sentences in data:\n","\n","        # transformam fiecare review in reprezentarea lui sub forma de indici ale caracterelor continute\n","        sentences_of_indices = [char_indices[w] if w in char_indices.keys() else char_indices['UNK'] for w in sentences]\n","\n","        # pentru fiecare indice putem face reprezentarea one-hot corespunzatoare\n","        # sau putem sa nu facem asta si sa adaugam un embedding layer in model care face această transformare\n","        if one_hot:\n","            sentences_of_indices = np.eye(len(char_indices))[sentences_of_indices]\n","\n","        vectorized.append(sentences_of_indices)\n","\n","    return vectorized\n","\n","train_reviews_vectorized = vectorize_sentences(train_reviews, char_indices)\n","# train_reviews_vectorized[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1647392866265,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"BVwubTwmgIqH","outputId":"d8c2ddbb-61c7-4f85-882f-250bba6cd19d"},"outputs":[],"source":["vectors_dim = [len(repr) for repr in train_reviews_vectorized]\n","vectors_dim[:5]"]},{"cell_type":"markdown","metadata":{"id":"m2z30TbuiFH8"},"source":["Putem vedea ca deoarece review-urile au numar diferit de caractere, in consecinta, si dimensiunile reprezentarilor vectoriale sunt diferite. \n","Vom aduce reprezentarile noastre la aceeasi dimensiune maxima.\n","\n","Definim o functie *pad* care:\n","\n","- primeste un set de review-uri si o lungime maxima\n","- scurteaza toate reprezentarile mai mari decat lungimea maxima\n","- adauga valoarea de padding (1 in cazul nostru) la reprezentarile mai scurte decat lungimea maxima"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1647392866268,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"R1NeqnXkXh0A"},"outputs":[],"source":["def pad(samples, max_length):\n","    \n","    return torch.tensor([\n","        sample[:max_length] + [1] * max(0, max_length - len(sample))\n","        for sample in samples\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1706,"status":"ok","timestamp":1647392867963,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"-xkuZPbUXJBR","outputId":"8a1e495a-0782-4e7d-8eea-ff28e325f688"},"outputs":[],"source":["train_reviews_vectorized = pad(train_reviews_vectorized, max_length = 1000)\n","train_reviews_vectorized.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1647392867964,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"wonME0qbHOdJ","outputId":"3b4cb0aa-be92-4718-8494-c61aa6522f3d"},"outputs":[],"source":["train_reviews_vectorized.shape"]},{"cell_type":"markdown","metadata":{"id":"T4UI32abSffV"},"source":["Tranformăm și review-urile din setul de test într-o reprezentare vectorială"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1794,"status":"ok","timestamp":1647392869753,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"CKT9Fq7wSjKY","outputId":"295f9c77-fb31-4478-ede4-2f4b19ef0c15"},"outputs":[],"source":["test_reviews_to_ascii = [unidecode(review) for review in test_df.review]\n","test_reviews = transform_to_char(test_reviews_to_ascii)\n","test_labels = test_df.sentiment.tolist()\n","\n","print('Reviews', len(test_reviews))\n","print('Labels', len(test_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1942,"status":"ok","timestamp":1647392871690,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"LvFzxZKYS8W0"},"outputs":[],"source":["test_reviews_vectorized = vectorize_sentences(test_reviews, char_indices)\n","test_reviews_vectorized = pad(test_reviews_vectorized, max_length = 1000)\n","# test_reviews_vectorized[0]"]},{"cell_type":"markdown","metadata":{"id":"nvD0iwNrH9Tu"},"source":["Vom incarca seturile noastre de date intr-un obiect din clasa [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1647392871692,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"tC2sg5M3H8mT"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, samples, labels):\n","        self.samples = samples\n","        self.labels = labels\n","            \n","    def __getitem__(self, k):\n","        \"\"\"Returneaza al k-lea exemplu din dataset\"\"\"\n","        return self.samples[k], self.labels[k]\n","    \n","    def __len__(self):\n","        \"\"\"Returneaza dimensiunea datasetului\"\"\"\n","        return len(self.samples)"]},{"cell_type":"markdown","metadata":{"id":"jB3HhhKKIeDW"},"source":["Definim arhitectura modelului"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1647392871694,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"SHQYbpUhQZXs"},"outputs":[],"source":["class Model(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Definim un embedding layer cu un vocabular de dimensiune 72\n","        # și ca output un embedding de dimensiune 20\n","        # padding_idx este indexul din vocabular al paddingului (1, în cazul nostru)\n","        \n","        self.embedding = torch.nn.Embedding(72, 20, padding_idx=1)\n","\n","        # Definim o secvență de layere\n","        \n","        # Un layer Convolutional 1D cu 20 input channels, 32 output channels, dimensiune kernel = 3 și padding = 1\n","        # ReLU activation\n","        # 1D Maxpooling layer de dimensiune 2\n","        conv1 = torch.nn.Sequential(\n","            torch.nn.Conv1d(in_channels=20, out_channels=32, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool1d(kernel_size=2),\n","        )\n","        \n","        # Un layer Convolutional 1D cu 32 input channels, 32 output channels, dimensiune kernel = 5 și padding = 2\n","        # ReLU activation\n","        # 1D Maxpooling layer de dimensiune 2\n","        conv2 = torch.nn.Sequential(\n","            torch.nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool1d(kernel_size=2),\n","        )\n","        \n","        # Global Average pooling layer care, în cazul nostru, este un 1D Avgerage Pooling layer\n","        # cu dimensiunea de 250 și stride 250\n","        global_average = torch.nn.AvgPool1d(kernel_size=250, stride=250)\n","\n","        self.convolutions = torch.nn.Sequential(\n","            conv1, conv2, global_average\n","        )\n","        \n","        # Flattening layer\n","        flatten = torch.nn.Flatten()\n","        \n","        # Linear layer cu 32 input features și 2 outputs fără funcție de activare\n","        linear = torch.nn.Linear(in_features=32, out_features=2)\n","\n","        self.classifier = torch.nn.Sequential(flatten, linear)\n","        \n","    def forward(self, input):\n","        # trecem inputul prin layerul de embedding\n","        embeddings = self.embedding(input)\n","        \n","        # permutăm inputul astfel încât prima dimensiune este numărul de channels\n","        embeddings = embeddings.permute(0, 2, 1)\n","        \n","        # trecem inputul prin secvența de layere\n","        output = self.convolutions(embeddings)\n","        output = self.classifier(output)\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1647392871695,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"miX4Qat9SBTK"},"outputs":[],"source":["DEVICE = torch.device(\"cuda\")\n","# instanțiem modelul\n","model = Model().to(DEVICE)\n","\n","# Adam optimizer cu lr = 1e-3\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Cross Entropy loss\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# training dataset and dataloader\n","# test dataset and dataloader\n","train_ds = Dataset(train_reviews_vectorized, train_labels)\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n","test_ds = Dataset(test_reviews_vectorized, test_labels)\n","test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"bVy9KScOVgCa"},"source":["Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15544,"status":"ok","timestamp":1647392887227,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"8jQzscISIdYG","outputId":"07a0585e-963c-48fd-86ab-25030e04cd6d"},"outputs":[],"source":["best_val_acc = 0\n","for epoch_n in range(10):\n","    print(f\"Epoch #{epoch_n + 1}\")\n","    model.train()\n","    for batch in train_dl:\n","        model.zero_grad()\n","\n","        inputs, targets = batch\n","        inputs = inputs.long().to(DEVICE)\n","        targets = targets.to(DEVICE)\n","\n","        output = model(inputs)\n","        loss = loss_fn(output, targets)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    # validare\n","    model.eval()\n","    all_predictions = torch.tensor([])\n","    all_targets = torch.tensor([])\n","    for batch in test_dl:\n","        inputs, targets = batch\n","        inputs = inputs.long().to(DEVICE)\n","        targets = targets.to(DEVICE)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        predictions = output.argmax(1)\n","        all_targets = torch.cat([all_targets, targets.detach().cpu()])\n","        all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n","\n","    val_acc = (all_predictions == all_targets).float().mean().numpy()\n","    print(val_acc)\n","\n","    if val_acc > best_val_acc:\n","        torch.save(model.state_dict(), \"./model\")\n","        best_val_acc = val_acc\n","\n","print(\"Best validation accuracy\", best_val_acc)"]},{"cell_type":"markdown","metadata":{"id":"XdrDykrMHAwF"},"source":["### Reprezentarea vectoriala la nivel de cuvant\n"]},{"cell_type":"markdown","metadata":{"id":"CXASSJD9E3wk"},"source":["\n","```\n","Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n","```\n","\n","Pe langa tokenii prezenti in textele noastre, adaugam si 2 tokeni speciali: UNK (unknown word) si PAD.\n","\n","\n","```\n","Index assigned for every token: {'UNK': 0, 'PAD': 1, 'the': 2, 'mouse': 3, 'ran': 4, 'up': 5, 'clock': 6, 'down': 7}\n","```\n","\n","Reprezentarea vectoriala a celor doua texte folosind indexul corespunzator pentru fiecare cuvant:\n","\n","```\n","'The mouse ran up the clock' = [2, 3, 4, 5, 2, 6]\n","'The mouse ran down' = [2, 3, 4, 7]\n","```\n","\n","Adaugam valori de padding la cel de-al doilea vector pentru a avea o lungime egala cu primul vector si obtinem:\n","\n","```\n","[2, 3, 4, 7, 1, 1]\n","```\n","Reprezentarea one-hot a fiecarui text:\n","\n","```\n","'The mouse ran up the clock' = [[0. 0. 1. 0. 0. 0. 0.]\n","                                [0. 0. 0. 1. 0. 0. 0.]\n","                                [0. 0. 0. 0. 1. 0. 0.]\n","                                [0. 0. 0. 0. 0. 1. 0.]\n","                                [0. 0. 1. 0. 0. 0. 0.]\n","                                [0. 0. 0. 0. 0. 0. 1.]]\n","\n","'The mouse ran down' = [[0. 0. 1. 0. 0. 0. 0. 0.]\n","                        [0. 0. 0. 1. 0. 0. 0. 0.]\n","                        [0. 0. 0. 0. 1. 0. 0. 0.]\n","                        [0. 0. 0. 0. 0. 0. 0. 1.]\n","                        [0. 1. 0. 0. 0. 0. 0. 0.]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"oG8exJuIYZVh"},"source":["Impartim textele din dataset in tokeni."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1647392887230,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"aKBCVNzETzUt"},"outputs":[],"source":["def transform_to_tokens(data):\n","\n","    reviews = []\n","    for review in data:\n","        review_tokenized = word_tokenize(review.lower())\n","        reviews.append(review_tokenized)\n","\n","    return reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13755,"status":"ok","timestamp":1647392900974,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"364y8q0KYGVy","outputId":"edb2e12b-414a-418a-bc31-10a5b5560beb"},"outputs":[],"source":["train_reviews = transform_to_tokens(train_df.review)\n","for r in train_reviews[:2]:\n","    print(r[:20])"]},{"cell_type":"markdown","metadata":{"id":"ufyvcVGLY2S8"},"source":["Construim vocabularul de tokeni"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1647392901521,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"TsV0rjqiY3Ta","outputId":"dfb2ec4a-c979-4aba-d20a-1461eecdf653"},"outputs":[],"source":["vocab = get_vocab(train_reviews)\n","\n","print('total words:', len(vocab))\n","print(list(vocab)[:100])"]},{"cell_type":"markdown","metadata":{"id":"ROYnhY5aZk2F"},"source":["Avem un vocabular foarte mare. Vom scoate din vocabular cuvintele cu o frecventa foarte mica. \n","\n","O alta abordare pentru micscorarea vocabularului este scoaterea cuvintelor foarte frecvente (stopwords). De asemenea putem face si alti pasi de preprocesare: putem face stemming, lematizare, putem scoate punctuatia, etc.)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1647392901523,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"suBSGbXEZ21G"},"outputs":[],"source":["import operator\n","\n","def word_freq(data, min_aparitions):\n","    \n","    all_words = [words.lower() for sentences in data for words in sentences]\n","    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n","    final_vocab = [k for k,v in sorted_vocab if v > min_aparitions]\n","\n","    return final_vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":604,"status":"ok","timestamp":1647392902120,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"mTvH9c6xah5I","outputId":"e7c72c3f-b971-42f0-ee24-bfd6b040ad4c"},"outputs":[],"source":["vocab = word_freq(train_reviews, min_aparitions = 10)\n","\n","print(vocab[:100])\n","print(len(vocab))"]},{"cell_type":"markdown","metadata":{"id":"EhIUcWG7Yeg-"},"source":["Fiecarui token ii atribuim un indice."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647392902121,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"ZH9CPMdib_VL"},"outputs":[],"source":["word_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n","indices_word = dict((i + 2, c) for i, c in enumerate(vocab))\n","\n","indices_word[0] = 'UNK'\n","word_indices['UNK'] = 0\n","\n","indices_word[1] = 'PAD'\n","word_indices['PAD'] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1647392902429,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"bdgbyjopXh8u"},"outputs":[],"source":["# print(indices_word)"]},{"cell_type":"markdown","metadata":{"id":"csIrKV2Mc9Fh"},"source":["Acum putem transforma propozitiile din datasetul nostru intr-o reprezentare vectoriala, in care vom avea pentru fiecare cuvant indicele corespunzator din vocabular."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":458,"status":"ok","timestamp":1647392902885,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"-cPX7pE-XiCF"},"outputs":[],"source":["train_reviews_vectorized = vectorize_sentences(train_reviews, word_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647392902887,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"vg4UHdQRdZzp","outputId":"cf0381fc-6c27-45af-8139-5b83a889d687"},"outputs":[],"source":["vectors_dim = [len(repr) for repr in train_reviews_vectorized]\n","vectors_dim[:5]"]},{"cell_type":"markdown","metadata":{"id":"kbXLorrNd5Zt"},"source":["Din nou, pentru ca textele au un numar diferit de cuvinte, trebuie sa aducem vectorii la o reprezentare de aceeași dimensiune."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":344,"status":"ok","timestamp":1647392903225,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"g-KN_NNedhzo","outputId":"8918bc04-7e55-4b5c-8bdc-2dfbdb901dae"},"outputs":[],"source":["train_reviews_vectorized = pad(train_reviews_vectorized, max_length = 512)\n","train_reviews_vectorized"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1647392903226,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"nzD9dy47d38U","outputId":"d606740b-733e-47c5-9ada-456d83caa732"},"outputs":[],"source":["train_reviews_vectorized.shape"]},{"cell_type":"markdown","metadata":{"id":"cP5P2WPwXLSy"},"source":["# TASK\n","## Deadline: 31 martie ora 23:59.\n","\n","Formular pentru trimiterea temei: https://forms.gle/Bznaciv2MTy4kVL47\n","\n","Folosind intreg datasetul de mai sus (IMDb reviews) implementati urmatoarele cerinte:\n","1. Impartiti setul de date in 80% train, 10% validare si 10% test\n","2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)\n","3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)\n","4. Implementati urmatoarea arhitectura:\n","    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n","    * un layer dropout cu probabilitate 0.4\n","    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n","    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)\n","5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat\n","6. Evaluati cel mai bun model obtinut pe datele de test.\n"]},{"cell_type":"markdown","metadata":{},"source":["1. Impartiti setul de date in 80% train, 10% validare si 10% test"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data: 40000\n","Test data:  5000\n","Val data:   5000\n"]}],"source":["# citim datele, si le convertim la un format mai usor\n","# de procesat\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","data = pd.read_csv('IMDB_Dataset.csv')\n","data = list(zip(data.review, data.sentiment))\n","\n","train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n","val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n","\n","print(f\"Train data: {len(train_data)}\")\n","print(f\"Test data:  {len(test_data)}\")\n","print(f\"Val data:   {len(val_data)}\")"]},{"cell_type":"markdown","metadata":{},"source":["2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from unidecode import unidecode\n","import re\n","import num2words\n","from typing import List, Tuple\n","\n","def remove_special_characters(s: str):\n","    \"\"\"\n","        Scoate toate caracterele ne-ascii, si tot ce nu e cuvant este\n","        inlocuit cu un spatiu.\n","    \"\"\"\n","    # scoatem caracterele speciale\n","    s = unidecode(s)\n","\n","    # inlocuim numerele\n","    s = re.sub(r'\\d+', lambda s: ' ' + num2words.num2words(s.group(), lang='en') + ' ', s)\n","    \n","    # convertim la lowercase\n","    s = s.lower()\n","\n","    # scoatem tot ce este punctuatie\n","    s = re.sub(r'\\W+', ' ', s)\n","    \n","    return s\n","\n","def remove_special_characters_arr(s: List[Tuple[str, int]]):\n","    return list(map(lambda s: (remove_special_characters(s[0]).split(), s[1]), s))"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train_fixed_str[0] = (['lame', 'plot', 'and', 'two', 'dimensional', 'script', 'made', 'characters', 'look', 'like', 'cardboard', 'cut', 'outs', 'needless', 'to', 'say', 'this', 'made', 'it', 'difficult', 'to', 'feel', 'empathy', 'for', 'any', 'of', 'the', 'characters', 'especially', 'the', 'fiance', 'he', 'looked', 'and', 'acted', 'more', 'like', 'a', 'cartoon', 'in', 'summary', 'i', 'guess', 'you', 'could', 'say', 'it', 'was', 'on', 'par', 'with', 'your', 'typical', 'made', 'for', 'tv', 'drama', 'it', 'uses', 'just', 'about', 'every', 'cliche', 'in', 'the', 'book', 'the', 'tortured', 'classical', 'musician', 'who', 'wants', 'to', 'break', 'out', 'and', 'play', 'salsa', 'the', 'free', 'spirited', 'fiancee', 'engaged', 'to', 'a', 'bean', 'counter', 'personality', 'she', 'doesn', 't', 'love', 'i', 'won', 't', 'list', 'them', 'or', 'else', 'it', 'would', 'be', 'a', 'spoiler', 'because', 'i', 'd', 'be', 'giving', 'away', 'the', 'whole', 'plot', 'the', 'dancing', 'was', 'ok', 'but', 'nothing', 'special', 'i', 've', 'seen', 'worse', 'three', 'stars', 'for', 'good', 'music', 'the', 'band', 'was', 'really', 'tight', 'i', 'saw', 'it', 'on', 'youtube', 'thankfully', 'i', 'didn', 't', 'pay', 'good', 'money', 'to', 'see', 'it', 'at', 'a', 'theater', 'i', 'm', 'still', 'a', 'little', 'shocked', 'at', 'how', 'many', 'great', 'reviews', 'this', 'movie', 'has', 'garnished'], 0)\n"]}],"source":["train_fixed_str = remove_special_characters_arr(train_data)\n","val_fixed_str = remove_special_characters_arr(val_data)\n","test_fixed_str = remove_special_characters_arr(test_data)\n","\n","print(f\"train_fixed_str[0] = {train_fixed_str[0]}\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["First items of vocab: ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'movie', 'for', 'with', 'but', 'film', 'one', 'you', 't', 'on', 'not', 'he', 'are', 'his', 'have', 'be']\n"]}],"source":["from collections import Counter\n","\n","# frecventa fiecarui cuvant in train\n","words_frq = Counter([s for point in train_fixed_str for s in point[0]])\n","\n","# vocabularul pe care il folosim\n","VOCAB_WORDS_SIZE = 10000\n","vocab = sorted(list(words_frq), key=lambda e: -words_frq[e])\n","# scadem 2 ca sa avem si 'unk' si 'pad'\n","vocab = vocab[:VOCAB_WORDS_SIZE - 2]\n","\n","word_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n","indices_word = dict((i + 2, c) for i, c in enumerate(vocab))\n","\n","indices_word[0] = 'UNK'\n","word_indices['UNK'] = 0\n","\n","indices_word[1] = 'PAD'\n","word_indices['PAD'] = 1\n","\n","print(f\"First items of vocab: {vocab[:30]}\")"]},{"cell_type":"markdown","metadata":{},"source":["3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train shape: torch.Size([40000, 500])\n","val shape: torch.Size([5000, 500])\n","test shape: torch.Size([5000, 500])\n"]}],"source":["import torch as th\n","\n","# spargem labelul de text\n","train_labels = list(map(lambda s: s[1], train_fixed_str))\n","val_labels = list(map(lambda s: s[1], val_fixed_str))\n","test_labels = list(map(lambda s: s[1], test_fixed_str))\n","train_str = list(map(lambda s: s[0], train_fixed_str))\n","val_str = list(map(lambda s: s[0], val_fixed_str))\n","test_str = list(map(lambda s: s[0], test_fixed_str))\n","\n","word_to_index = lambda s: word_indices[s] if s in word_indices else 0\n","MAX_TOKENS = 500\n","\n","def fix_dataset(s: List[List[str]]):\n","    fix_str = lambda s: s[:min(len(s), MAX_TOKENS)] + ['PAD'] * max(0, MAX_TOKENS - len(s))\n","    s = map(fix_str, s)\n","    s = [list(map(word_to_index, i)) for i in s]\n","    return th.tensor(s)\n","\n","train_tokenized = fix_dataset(train_str)\n","val_tokenized = fix_dataset(val_str)\n","test_tokenized = fix_dataset(test_str)\n","\n","print(f\"train shape: {train_tokenized.shape}\")\n","print(f\"val shape: {val_tokenized.shape}\")\n","print(f\"test shape: {test_tokenized.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["4. Implementati urmatoarea arhitectura:\n","    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n","    * un layer dropout cu probabilitate 0.4\n","    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n","    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.embedding = th.nn.Embedding(VOCAB_WORDS_SIZE, 100, padding_idx=1)\n","\n","        self.net = nn.Sequential(\n","            # primul strat de nebunii\n","            nn.Dropout(0.4),\n","            nn.Conv1d(100, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","            nn.MaxPool1d(2),\n","\n","            # al doilea strat\n","            nn.Conv1d(128, 128, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","            nn.MaxPool1d(2),\n","\n","            # al treilea strat\n","            nn.Conv1d(128, 128, kernel_size=5, padding=2),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","            nn.MaxPool1d(2),\n","\n","            nn.AvgPool1d(62),\n","\n","            nn.Flatten(),\n","            nn.Linear(128, 2)\n","        )\n","        \n","    def forward(self, input):\n","        output = self.embedding(input)\n","        output = output.permute(0, 2, 1)\n","        output = self.net(output)\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# generam dataloaderele\n","train_dataset = th.utils.data.TensorDataset(train_tokenized, th.tensor(train_labels))\n","train_dataloader = th.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n","\n","val_dataset = th.utils.data.TensorDataset(val_tokenized, th.tensor(val_labels))\n","val_dataloader = th.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n","\n","test_dataset = th.utils.data.TensorDataset(test_tokenized, th.tensor(test_labels))\n","test_dataloader = th.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["DEVICE = th.device('cuda')\n","\n","net = Model().to(DEVICE)\n","\n","optimizer = th.optim.Adam(params=net.parameters())\n","loss_fn = nn.CrossEntropyLoss()\n","\n","best_val_acc = 0.\n","best_net = None"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch #1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:11<00:00, 26.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.83\n","Epoch #2\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 24.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.7554\n","Epoch #3\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 24.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.8428\n","Epoch #4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 24.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.886\n","Epoch #5\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 24.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.8928\n","Epoch #6\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 25.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.8542\n","Epoch #7\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 25.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.9014\n","Epoch #8\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 24.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.8822\n","Epoch #9\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 25.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.9056\n","Epoch #10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 313/313 [00:12<00:00, 24.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["0.7564\n","Best validation accuracy 0.9056\n"]}],"source":["import copy\n","from tqdm import tqdm\n","\n","for epoch_n in range(10):\n","    print(f\"Epoch #{epoch_n + 1}\")\n","    net.train()\n","    for batch in tqdm(train_dataloader):\n","        net.zero_grad()\n","\n","        inputs, targets = batch\n","        inputs = inputs.long().to(DEVICE)\n","        targets = targets.to(DEVICE)\n","\n","        output = net(inputs)\n","        loss = loss_fn(output, targets)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    # validare\n","    net.eval()\n","    all_predictions = th.tensor([])\n","    all_targets = th.tensor([])\n","    for batch in val_dataloader:\n","        inputs, targets = batch\n","        inputs = inputs.long().to(DEVICE)\n","        targets = targets.to(DEVICE)\n","\n","        with th.no_grad():\n","            output = net(inputs)\n","\n","        predictions = output.argmax(1)\n","        all_targets = th.cat([all_targets, targets.detach().cpu()])\n","        all_predictions = th.cat([all_predictions, predictions.detach().cpu()])\n","\n","    val_acc = (all_predictions == all_targets).float().mean().numpy()\n","    print(val_acc)\n","\n","    if val_acc > best_val_acc:\n","        best_net = copy.deepcopy(net.cpu())\n","        net.to(DEVICE)\n","        best_val_acc = val_acc\n","\n","print(\"Best validation accuracy\", best_val_acc)"]},{"cell_type":"markdown","metadata":{},"source":["6. Evaluati cel mai bun model obtinut pe datele de test."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9148\n","Precision: 0.9150296484499778\n","Recall: 0.9150623297072501\n","F1: 0.9147998773118233\n","\n"]}],"source":["import sklearn.metrics as metrics\n","\n","best_net.to(DEVICE)\n","best_net.eval()\n","all_predictions = th.tensor([])\n","all_targets = th.tensor([])\n","for batch in test_dataloader:\n","    inputs, targets = batch\n","    inputs = inputs.long().to(DEVICE)\n","    targets = targets.to(DEVICE)\n","\n","    with th.no_grad():\n","        output = best_net(inputs)\n","\n","    predictions = output.argmax(1)\n","    all_targets = th.cat([all_targets, targets.detach().cpu()])\n","    all_predictions = th.cat([all_predictions, predictions.detach().cpu()])\n","\n","\n","print(\n","    f\"Accuracy: {metrics.accuracy_score(all_targets, all_predictions)}\\n\"\n","    f\"Precision: {metrics.precision_score(all_targets, all_predictions, average='macro')}\\n\"\n","    f\"Recall: {metrics.recall_score(all_targets, all_predictions, average='macro')}\\n\"\n","    f\"F1: {metrics.f1_score(all_targets, all_predictions, average='macro')}\\n\"\n",")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# salvam modelul\n","th.save(best_net.state_dict(), \"./model\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"FINAL_lab_5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":0}
